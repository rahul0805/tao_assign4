{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics in Applied Optimization (IIIT, Hyderabad, India)\n",
    "# Jupyter Notebook Assignment\n",
    "### Instructor: Dr. Pawan Kumar (IIIT, H) (https://faculty.iiit.ac.in/~pawan.kumar/)\n",
    "If you are not familiar with Jupyter notebook, before proceeding further, please go and watch this video:\n",
    "\n",
    "https://www.youtube.com/watch?v=HW29067qVWk\n",
    "\n",
    "## Regarding assignments\n",
    "### Deadline: 30 November 2020 \n",
    "- all the assignment is to be done in this notebook itself\n",
    "- any proof etc can be done on paper, and image is to be inserted in this notebook\n",
    "- save this notebook with your roll number and upload it in moodle\n",
    "- basic familiarity with python is required, brush up if necessary\n",
    "- you are not allowed to use any existing library for gradient methods; this defeats the purpose of this assignment\n",
    "- sample output is in the zip file of assignment\n",
    "- if there are any doubt, then raise it in course moodle site, it may help others\n",
    "- if you find typo, raise this issue in moodle promptly!\n",
    "- please avoid copying from others, there may be oral exam to test your knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Recommender System Using Latent Factor Model in Python\n",
    "\n",
    "Please refer to the class slides for more detail. Also, for full detail, please refer the following main reference:\n",
    "\n",
    "**[1] Y. Koren, R. Bell, and C. Volinsky, Matrix Factorization Techniques for Recommender Systems, Computer Archive, Volume 42, Issue 8, 2009** \n",
    "\n",
    "We first load the necessary libraries: \n",
    "1. matplotlib: needed for plotting figures and \n",
    "2. numpy needed for doing math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we write the main functions\n",
    "\n",
    "We write everything inside a class LF, which stands for latent factor. We need the following functions:\n",
    "1. main\n",
    "2. train\n",
    "3. mse\n",
    "4. sgd\n",
    "5. predict\n",
    "6. full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Tutorial on Recommender Systems\n",
    "Please see lecture slides for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Factor Model\n",
    "In a recommender systems, we are given an incomplete ratings matrix say R. The $(i,j)$ entry of this matrix is denoted by $r_{ij}.$ \n",
    "- We believe that there are dependencies in the matrix, i.e., the matrix is low rank, hence it can be written as a product of low rank matrices. \n",
    "- Even if the matrix is not strictly low rank, we believe that certain features are more important than others. In other words, if the ratings matrix were full, then after doing SVD, we get \n",
    "$$R = U S V^T$$\n",
    "some of the singular values were very small, hence the matrix can be approximated well with truncated SVD. Let us consider the first $k$ largest singular vectors, then the truncated SVD is \n",
    "$$R \\approx U_k S_k V_k.$$\n",
    "- See IPSC notes for more detail on how to compute SVD or truncated SVD\n",
    "- For a recomendation problem, for example, movie recomendation or news recommendation or product recommendation, etc, we believe that certain latent features are most important and play a major role in ratings.\n",
    "- For movie recommendation, for example, latent factors could be comedy, adventure, horror, etc. These features are called latent, because the given data, which is ratings matrix does not explicitely tells about these.\n",
    "- For recommendation systems, we cannot do SVD, because the matrix $R$ is incomplete! Obviously, we cant treat the missing entries to be zero! Hence, instead of doing SVD, we will consider an error function that consider only the ratings that are given. Let us assume that the ratings matrix can be modelled as a latent factors, i.e., suppose that $$R = PQ, \\quad P \\in \\mathbb{R}^{n \\times k}, Q \\in \\mathbb{R}^{k \\times n},$$ then the given rating $$r_{ij} = p_{i*}q_{*j},$$ where $p_{i*}$ denotes the $i$th row of $P,$ and $q_{*j}$ denotes the $j$th column of $Q.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model\n",
    "- As mentioned above, we cannot directly do SVD, however, we can indeed create a **loss function** as follows:\n",
    "$$\\mathcal{L}(p_{1*}, p_{2*}, \\dots p_{n*}, q_{*1}, q_{*2}, \\dots, q_{*n}) = \\sum_{(i,j) \\in K} (r_{ij} - p_{i*}q_{*j}).$$\n",
    "- Note above that we see loss function as a function the variables $p_{1*}, p_{2*}, \\dots, p_{*n},$ which are row vectors of $P,$ and as the column vectors of $Q,$ which are $q_{*1}, q_{*2}, \\dots, q_{*n}.$ This allows us to **vectorize**. Why vectorize?\n",
    "- In machine learning the weights and their combinations with the given data creates a model. In this case, the model is $PQ.$ Since the $k,$ is a hyperparameter, it is likely that one may choose $P$ and $Q$ to have too many columns, i.e., too many weights are used, this will lead to a large model, and hence leading to **overfitting**. \n",
    "If you are not familiar with overfitting, then pause here, and see the video here:\n",
    "\n",
    "https://www.youtube.com/watch?v=u73PU6Qwl1I\n",
    "\n",
    "    - What did you learn from video? \n",
    "    - What is termed as model? \n",
    "    - What is the meaning of model being large? \n",
    "    - What is the difference between linear and logistic regression? \n",
    "    - What is underfitting? \n",
    "    - What is a bias? \n",
    "    - How do you characterize bias and variance using terms overfitting or underfitting?\n",
    "\n",
    "- A well known way to avoid overfitting is to do regularizations, by penalizing **large model** size. Let us modify our loss function as follows:\n",
    "$$\\mathcal{L}(\\cdot) = \\sum_{(i,j) \\in K} (r_{ij} - p_{i*}q_{*j})^2 + \\gamma/2 (\\| P \\|_F^2 + \\| Q \\|_F^2)$$\n",
    "    - We have added the term $\\gamma/2(\\| P \\|_F^2 + \\| Q \\|_F^2).$ It is called **regularization term**\n",
    "    - The parameter $\\gamma$ is called the **regularization parameter**\n",
    "    - High $\\gamma$ would mean that we don't want big model size; this may help prevent overfitting\n",
    "- Let $e_{ij}$ denote the error corresponding to each $(ij)$ term, \n",
    "$$e_{ij} = r_{ij} - \\sum_{s=1}^k p_{is}q_{sj}$$\n",
    "then let us define the new error term $\\tilde{e}$\n",
    "$$\\tilde{e}_{ij} = e_{ij}^2 + \\gamma/2 (\\|P\\|_F^2 + \\| Q \\|_F^2),$$ \n",
    "that is,\n",
    "$$\\tilde{e}_{ij} = (r_{ij} - \\sum_{s=1}^k p_{is}q_{sj} )^2 + \\gamma/2 (\\|P\\|_F^2 + \\| Q \\|_F^2)$$\n",
    "- Note that $$\\mathcal{L}(\\cdot) = \\sum_{(i,j) \\in \\mathcal{K}} \\tilde{e}_{ij},$$\n",
    "where $\\mathcal{K}$ is the set of all indices for which ratings $r_{ij}$ are available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Method Versus Stochastic Gradient Methods\n",
    "Following notes are taken from \n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "For large dataset, $k$ and $n$ tend to be large, and computing gradient fully becomes too demanding. In machine learning or statistical estimation, we are usually required to find minimum of the loss functions of the form \n",
    "$$Q(w) = 1/n \\sum_{j=1}^n Q_j (w)$$\n",
    "That is loss function as \"additive\" decomposition. In other words, the loss function is a sum of the loss function where the sum is over the data samples. If $n$ is large, then computing full gradient is computationally demanding. However, if we indeed decide to run the gradient method. Then we recall the following steps:\n",
    "1. Take initial random weight (possibly random and normally distributed between 0 and 1): $w^0$\n",
    "2. $w^{i+1} = w^i - \\alpha \\nabla Q(w^i), \\quad i=0, \\dots$\n",
    "\n",
    "Here the gradient $\\nabla Q$ is the full gradient in the sense that\n",
    "\\begin{align}\n",
    "\\nabla Q(w) = \\sum_{i=1}^n \\nabla_{w}Q_i(w). \\label{grad} \\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "Here\n",
    "\\begin{align}\n",
    "\\nabla_w Q_i (w) = \\begin{bmatrix}\n",
    "\\dfrac{\\partial Q_i (w)}{\\partial w_1} \\\\\n",
    "\\dfrac{\\partial Q_i (w)}{\\partial w_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial Q_i (w)}{\\partial w_n} \\\\\n",
    "\\end{bmatrix}  \\label{grad2} \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "### Stochastic gradient  descent method\n",
    "Stochastic gradient merthod, in short, SGD is a method (not necessarily descent due to stochasticity, but in expectation is usually is a descent), where the gradient is computed partially. For example, instead of summing for all $i=1...n,$ we can take a random sample, say $j_1,$ and do the update only for this sample as follows:\n",
    "$$\\nabla Q(w) = \\nabla_{w}Q_{j_1}(w),$$\n",
    "that is we have picked just one of the term i.e., $j_1$th term from \\eqref{grad}.\n",
    "Then the SGD update is:\n",
    "$$w^{i+1} = w^i - \\alpha \\nabla Q_{j_1}(w^i), \\quad i=0, \\dots$$\n",
    "Note that we could have taken few more terms in the sum, and would have updated for a batch. This is called, batched stochastic gradient descent.\n",
    "It sounds incorrect, isnt it? Why such method may converge? With some basic assumption on the loss function, we can prove some convergence in expectance. More on convergence theory in class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-0:}}$ Compute the full gradient of the loss function for recommendation system\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we derive a stochastic gradient for the loss function for recommender loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Method for Optimization Problem\n",
    "- We want to use gradient method to minimize the error. An update in gradient method is given as follows\n",
    "$$w^{i+1} = w^i - \\nabla \\mathcal{L}(w^i)$$\n",
    "- To use gradient method, we need to compute gradient. If we see $\\mathcal{L}$ as a function of $p_{ik}'$s and $q_{ki}'$s, then \n",
    "$$\\dfrac{\\partial \\mathcal{L}}{\\partial p_{is}} = 2e_{is} \\dfrac{\\partial e_{is}}{\\partial p_{is}} + \\gamma p_{is} = -2e_{is} q_{sj} + \\gamma p_{is}$$\n",
    "Similarly,\n",
    "$$\\dfrac{\\partial \\mathcal{L}}{\\partial q_{sj}} = 2e_{sj} \\dfrac{\\partial e_{sj}}{\\partial q_{sj}}+\\gamma q_{sj} = -2e_{sj}p_{js} + \\gamma q_{sj},$$\n",
    "- The gradient vector $\\nabla \\mathcal{L}$ is given by\n",
    "$$\\nabla \\mathcal{L} = \\left(\\dfrac{\\partial \\mathcal{L}}{\\partial p_{11}}, \\dots, \\dfrac{\\partial \\mathcal{L}}{\\partial p_{nk}}, \\dfrac{\\partial \\mathcal{L}}{\\partial q_{11}}, \\dots, \\dfrac{\\partial \\mathcal{L}}{\\partial p_{kn}} \\right)$$\n",
    "- The weights $p_{is}$ can be updated as\n",
    "$$p_{is} = p_{is} + \\alpha (2e_{ij} q_{sj} - \\gamma p_{is}) $$\n",
    "- The weights $p_{sj}$ can be updated as\n",
    "$$q_{sj} = q_{sj} + \\alpha (2e_{ij} p_{js} - \\gamma q_{sj}) $$\n",
    "- We can vectorize $s.$ Finally, the vectorized form of the update looks like:\n",
    "$$(p_{i+1*}, q_{*j+1})^T = (p_{i*}, q_{*j})^T + \\alpha ((2e_{ij} q_{*j} - \\gamma p_{i*}), (2e_{ij} p_{j*} - \\gamma q_{*j}))^T  $$\n",
    "- The error at $(p_{i*}, q_{*j})^T$ nedded above can be computed by $$e_{ij} =  r_{ij} - \\text{prediction at}~ (i,j) $$\n",
    "Here prediction at $(i,j)$ can be computed by making a call to predict() function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-1:}}$ Why the error is computed this way?\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Either use latex (preferred) or write it on paper, take pic, and insert it here. See the video on jupyter above on how to paste images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-2:}}$ Are the updates for $p_{i*}$ and $q_{*j}$ stochastic? Justify.\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Bias\n",
    "It is often observed in practice that adding a bias term helps. Let us add the bias term $b$ to the prediction function. The bias term is computed as the mean of the ratings matrix. See the predict function below to see how bias is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-2b:}}$ Read the main reference paper and justify why bias is added. Note in this notebook we add global bias. Which other bias term was suggested in the paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Answer:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LF(): \n",
    "    \n",
    "    def __init__(self, R, K, alpha, gamma, iterations):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - gamma (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Compute the global bias\n",
    "        # For Question-6, you may want to initialize additional bias terms here\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        #self.b_i = TODO \n",
    "        #self.b_u = TODO\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"SGD Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return training_process\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        MSE: Compute Mean Square Error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        SGD: Stochastic Graident Descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Compute prediction and error\n",
    "            prediction = TODO\n",
    "            e = TODO\n",
    "            \n",
    "            # Keep a copy of row of P. It will be needed for update\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += TODO\n",
    "            self.Q[j, :] += TODO\n",
    "            \n",
    "            \n",
    "    def gd(self):\n",
    "        \"\"\"\n",
    "        GD: Graident Descent\n",
    "        Replace self.sample below to full batch for gradient descent\n",
    "        Feel free to remove everything, and implement from scratch\n",
    "        \"\"\"\n",
    "        for i, j, r in TODO:\n",
    "            # Compute prediction and error\n",
    "            prediction = TODO\n",
    "            e = TODO\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += TODO\n",
    "            self.Q[j, :] += TODO\n",
    "            \n",
    "\n",
    "    def predict(self, i, j):\n",
    "        \"\"\"\n",
    "        Prediction: Predicted the rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.P[i, :].dot(self.Q[j, :].T)\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        The rating matrix using the biases P and Q\n",
    "        \"\"\"\n",
    "        return lf.b + lf.P.dot(lf.Q.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])\n",
    "\n",
    "lf = LF(R, K=2, alpha=0.1, gamma=0.01, iterations=100)\n",
    "training_process = lf.train()\n",
    "print()\n",
    "print(\"P x Q:\")\n",
    "print(lf.full_matrix())\n",
    "print()\n",
    "print(\"Global bias:\")\n",
    "print(lf.b)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-3:}}$ Implement the full gradient descent method\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Complete gd function above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-4:}}$ Vectorize the updates of p and q, by vectorizing s (detailed above)\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Fill the TODO in code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the code below, answer the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x for x, y in training_process]\n",
    "y = [y for x, y in training_process]\n",
    "plt.figure(figsize=((16,4)))\n",
    "plt.plot(x, y)\n",
    "plt.xticks(x, x)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-5:}}$ Plot the MSE versus iterations for $\\alpha=1, \\gamma=0.01$ and 100 iterations of SGD\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Put your figure here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-6:}}$ Show the plot for 50 iterations, $\\alpha=1, \\gamma=0.01$ of SGD without bias, i.e., $b=0$\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Paste your output and figures here or below this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-7:}}$ Repeat above with bias\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Paste your output and figures here or below this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-8:}}$ Show plots with values of regularization parameters to be 1, 0.1, 0.01, 0.001 and for 50 iterations, and $\\alpha=1, \\gamma=0.01$\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Paste all your plots here or below this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-9:}}$ Prove that the loss function is not convex. Is the loss function differentiable?\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-10:}}$ In the reference paper [1] above, additional bias terms are recommended, implement it\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Put your modified function here or below this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-11:}}$ Download the netflix ratings matrix, and run your algorithm.\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Put your MSE versus iterations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thats All Folks! Further Readings or Infomation \n",
    "https://www.coursera.org/learn/networks-illustrated/lecture/8GPZT/netflix-timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep Recommending!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
